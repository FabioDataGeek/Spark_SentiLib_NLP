{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6ff2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/fabio/anaconda3/lib/python3.9/site-packages (1.21.5)\r\n"
     ]
    }
   ],
   "source": [
    "# Importamos todas las librerías necesarias para utilizar Spark Streaming con Python desde\n",
    "# JUpyter Notebook\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "!pip3 install numpy\n",
    "import numpy\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a0aa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 18:59:52,713 WARN util.Utils: Your hostname, fabio resolves to a loopback address: 127.0.1.1; using 192.168.100.23 instead (on interface eno1)\n",
      "2022-03-30 18:59:52,714 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/fabio/.ivy2/cache\n",
      "The jars for the packages stored in: /home/fabio/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9d97203b-dd2d-41fb-acad-c6da7fe8245c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;3.4.2 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.5.3 in central\n",
      "\tfound joda-time#joda-time;2.9.5 in central\n",
      "\tfound org.joda#joda-convert;1.8.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      "\tfound net.sf.trove4j#trove4j;3.0.3 in central\n",
      ":: resolution report :: resolve 340ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;3.4.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.5 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n",
      "\torg.joda#joda-convert;1.8.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.5.3 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   0   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9d97203b-dd2d-41fb-acad-c6da7fe8245c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/8ms)\n",
      "2022-03-30 18:59:53,490 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Iniciamos la sesión de Spark indicando el master empleado y la configuración utilizada\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7372c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 19:00:07.468197: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2022-03-30 19:00:07.468216: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "from SentiLib.text import predict_emotions_text\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "class CustomTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "  \n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "  \n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "  \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "        # The custom action: concatenate the integer form of the doubles from the Vector\n",
    "        transform_udf = F.udf(lambda x: str(predict_emotions_text(x)), StringType())\n",
    "        return df.withColumn(output_col, transform_udf(input_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde66fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n",
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "# Tomamos de la carpeta \"Datos\" los archivos txt que van entrando con las diferentes frases\n",
    "# Estos archivos se toman de uno en uno.\n",
    "text = spark.readStream \\\n",
    ".format(\"text\") \\\n",
    ".text(\"file:///home/user/Datos\")\n",
    "print(sparknlp.version())\n",
    "print(spark.version)\n",
    "Document = DocumentAssembler().setInputCol('value').setOutputCol('document').setCleanupMode('shrink')\n",
    "Sentence = SentenceDetector().setInputCols('document').setOutputCol('sentence')\n",
    "Sentence.setExplodeSentences(True)\n",
    "Tokenizer = RegexTokenizer().setInputCols('sentence').setOutputCol('token')\n",
    "Normalizer = Normalizer().setInputCols(['token']).setOutputCol('normalize').setLowercase(True).setCleanupPatterns([])\n",
    "Stemmer = Stemmer().setInputCols('normalize').setOutputCol('stemming')\n",
    "TokenAssembler = TokenAssembler().setInputCols(\"sentence\", \"stemming\").setOutputCol(\"cleanText\")\n",
    "Finisher = Finisher().setInputCols('cleanText').setOutputCols('output').setOutputAsArray(False)\n",
    "custom_transformer = CustomTransformer(input_col=\"output\", output_col=\"sentiment\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    Document,\n",
    "    Sentence,\n",
    "    Tokenizer,\n",
    "    Normalizer,\n",
    "    Stemmer,\n",
    "    TokenAssembler,\n",
    "    Finisher,\n",
    "    custom_transformer\n",
    "])\n",
    "\n",
    "model = pipeline.fit(text)\n",
    "result = model.transform(text)\n",
    "\n",
    "final = result.withColumn(\"timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a4721",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-30 19:24:31.880612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 19:24:31,900 WARN streaming.ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3e52494f-b9bc-49c8-883f-5084451442b6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "2022-03-30 19:24:31,908 WARN streaming.ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "2022-03-30 19:24:36.342157: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2022-03-30 19:24:36.342179: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "\n",
    "query =final.select('sentiment', 'timestamp') \\\n",
    ".writeStream \\\n",
    ".outputMode(\"append\") \\\n",
    ".format(\"console\") \\\n",
    ".option(\"maxFilesPerTrigger\", 1) \\\n",
    ".trigger(once=True) \\\n",
    ".start().awaitTermination()\n",
    "\n",
    "print(datetime.now())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
