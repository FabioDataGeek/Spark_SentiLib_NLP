{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09810542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/fabio/anaconda3/lib/python3.9/site-packages (1.21.5)\r\n"
     ]
    }
   ],
   "source": [
    "# Importamos todas las librerías necesarias para utilizar Spark Streaming con Python desde\n",
    "# JUpyter Notebook\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "!pip3 install numpy\n",
    "import numpy\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192f767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 20:25:17.996348: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2022-03-16 20:25:17.996370: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Importamos todas las librerías que vamos a necesitar para ejecutar los Scripts con Spark\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "import ast\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "\n",
    "englishST = open('/home/user/englishST.txt', 'r', encoding = 'latin-1')\n",
    "stop = englishST.read().splitlines()\n",
    "englishST.close()\n",
    "\n",
    "sb_english = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff52932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos los scripts que se utilizarán con las transformaciones de Spark\n",
    "\n",
    "# 1º Eliminación de las \"Regular Expressions\"\n",
    "def regex(text):\n",
    "    new_text = re.sub(\"\\W+\", \" \", text)#elimina puntos comas,etc\n",
    "    new_text = re.sub(\"\\d+\", \" \", new_text).strip()#elimina numeros\n",
    "    return new_text\n",
    "    \n",
    "   \n",
    "# 2º Ponemos todo el contenido en minúscula\n",
    "def lower_case(text): \n",
    "    new_text = linea.lower()\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# 3º Tokenización de las oraciones\n",
    "def tokenizer(text):\n",
    "    new_text = word_tokenize(text, language=\"english\")\n",
    "    return new_text\n",
    "\n",
    "# 4º Eliminamos las StopWords\n",
    "def stop_words(text):\n",
    "    text = text.splitlines()\n",
    "    for words in text:\n",
    "        words = ast.literal_eval(words)\n",
    "        new_sentence = []\n",
    "\n",
    "        for word in words:\n",
    "            if word not in stop:\n",
    "                new_sentence.append(word)\n",
    "\n",
    "        if new_sentence:\n",
    "            return new_sentence\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "def filter(text):\n",
    "\n",
    "    if text != []:\n",
    "        return text\n",
    "\n",
    "    \n",
    "# 5º Stemming\n",
    "def stemmer(text):\n",
    "    # Obtenemos el lemma por cada token\n",
    "    text = text.splitlines()\n",
    "    for words in text:\n",
    "        sentence = []\n",
    "        words = ast.literal_eval(words)\n",
    "        for word in words:\n",
    "            sentence.append(sb_english.stem(word))\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# 6º Lemmatización\n",
    "def lemmatization(text):\n",
    "    text = text.splitlines()\n",
    "    for words in text:\n",
    "        sentence = []\n",
    "        words = ast.literal_eval(words)\n",
    "\n",
    "    # obtenemos los lemmas\n",
    "    doc = nlp(\" \".join(words))\n",
    "    for word in doc:\n",
    "        sentence.append(word.lemma_)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# 7º Volvemos a formar la oración a partir de los tokens\n",
    "def join_all(text):\n",
    "\n",
    "    text = text.splitlines()\n",
    "    for words in text:\n",
    "        words = ast.literal_eval(words)\t\n",
    "        new_text = \" \".join(words)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e70d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 16:20:12.839407: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2022-03-28 16:20:12.839434: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.945544242858887\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "from SentiLib.text import predict_emotions_text\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "class CustomTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "  \n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "  \n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "  \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "        # The custom action: concatenate the integer form of the doubles from the Vector\n",
    "        transform_udf = F.udf(lambda x: str(predict_emotions_text(x)), StringType())\n",
    "        return df.withColumn(output_col, transform_udf(input_col))\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad45aeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 16:20:31,333 WARN util.Utils: Your hostname, fabio resolves to a loopback address: 127.0.1.1; using 192.168.100.23 instead (on interface eno1)\n",
      "2022-03-28 16:20:31,334 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/fabio/.ivy2/cache\n",
      "The jars for the packages stored in: /home/fabio/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6cfdab31-ff08-4c94-a6b7-485ce8332aeb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;3.4.2 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.5.3 in central\n",
      "\tfound joda-time#joda-time;2.9.5 in central\n",
      "\tfound org.joda#joda-convert;1.8.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      "\tfound net.sf.trove4j#trove4j;3.0.3 in central\n",
      ":: resolution report :: resolve 317ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;3.4.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.5 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n",
      "\torg.joda#joda-convert;1.8.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.5.3 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   0   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6cfdab31-ff08-4c94-a6b7-485ce8332aeb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/8ms)\n",
      "2022-03-28 16:20:32,082 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n",
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "# Creamos los pipelines con Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "#spark = sparknlp.start()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Tomamos de la carpeta \"Datos\" los archivos txt que van entrando con las diferentes frases\n",
    "# Estos archivos se toman de uno en uno.\n",
    "text = spark.read \\\n",
    ".format(\"text\") \\\n",
    ".text(\"file:///home/user/Datos/texto.txt\")\n",
    "\n",
    "\n",
    "print(sparknlp.version())\n",
    "print(spark.version)\n",
    "Document = DocumentAssembler().setInputCol('value').setOutputCol('document').setCleanupMode('shrink')\n",
    "Sentence = SentenceDetector().setInputCols('document').setOutputCol('sentence')\n",
    "Sentence.setExplodeSentences(True)\n",
    "Tokenizer = RegexTokenizer().setInputCols('sentence').setOutputCol('token')\n",
    "Normalizer = Normalizer().setInputCols(['token']).setOutputCol('normalize').setLowercase(True).setCleanupPatterns([])\n",
    "Stemmer = Stemmer().setInputCols('normalize').setOutputCol('stemming')\n",
    "TokenAssembler = TokenAssembler().setInputCols(\"sentence\", \"stemming\").setOutputCol(\"cleanText\")\n",
    "Finisher = Finisher().setInputCols('cleanText').setOutputCols('output').setOutputAsArray(False)\n",
    "custom_transformer = CustomTransformer(input_col=\"output\", output_col=\"sentiment\")\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    Document,\n",
    "    Sentence,\n",
    "    Tokenizer,\n",
    "    Normalizer,\n",
    "    Stemmer,\n",
    "    TokenAssembler,\n",
    "    Finisher,\n",
    "    custom_transformer\n",
    "])\n",
    "\n",
    "model = pipeline.fit(text)\n",
    "result = model.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a250536f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 16:20:50.989792: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2022-03-28 16:20:50.989810: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               value|              output|           sentiment|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|THEY ENJOY IT WHE...|thei enjoi it whe...|{'anger': 0, 'ant...|\n",
      "|THE VIEWPOINT OVE...|the viewpoint ove...|{'anger': 0, 'ant...|\n",
      "|THE DIAGNOSIS WAS...|the diagnosi wa d...|{'anger': 0, 'ant...|\n",
      "|RIGHT NOW MAY NOT...|right now mai not...|{'anger': 0, 'ant...|\n",
      "|THE CARPETCLEAN A...|the carpetclean a...|{'anger': 0, 'ant...|\n",
      "|AGRICULTURAL PROD...|agricultur produc...|{'anger': 0, 'ant...|\n",
      "|NO MANUFACTURER H...|no manufactur ha ...|{'anger': 1, 'ant...|\n",
      "|KINDERGART AN CHI...|kindergart an chi...|{'anger': 0, 'ant...|\n",
      "|DON'T ASK ME TO C...|don't ask me to c...|{'anger': 1, 'ant...|\n",
      "|THE NEAREST SYNAG...|the nearest synag...|{'anger': 0, 'ant...|\n",
      "|WILL YOU TELL ME WHY|will you tell me why|{'anger': 0, 'ant...|\n",
      "|STRAW HATS ARE OU...|straw hat ar out ...|{'anger': 0, 'ant...|\n",
      "|THE EASTERN COAST...|the eastern coast...|{'anger': 0, 'ant...|\n",
      "|HOWEVER THE LITTE...|howev the litter ...|{'anger': 0, 'ant...|\n",
      "|THE TOOTH FAIRY F...|the tooth fairi f...|{'anger': 1, 'ant...|\n",
      "|WILL YOU TELL ME WHY|will you tell me why|{'anger': 0, 'ant...|\n",
      "|WILL YOU TELL ME WHY|will you tell me why|{'anger': 0, 'ant...|\n",
      "|SHE HAD YOUR DARK...|she had your dark...|{'anger': 0, 'ant...|\n",
      "|HIS SHOULDER FELT...|hi shoulder felt ...|{'anger': 0, 'ant...|\n",
      "|CALCIOM MAKES BON...|calciom make bone...|{'anger': 0, 'ant...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1884.72114944458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Transformaciones y acciones\n",
    "start = time.time()\n",
    "result.show()\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
